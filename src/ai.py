import logging
import openai
from colorama import Fore, Style
import tiktoken
from config import OPENAI_API_KEY, DEAFAULT_MODEL

# Initialize logging
logging.basicConfig(format=f'{Fore.LIGHTBLACK_EX}%(message)s{Style.RESET_ALL}', level=logging.INFO)

openai.api_key = OPENAI_API_KEY


class AI:
    def __init__(self, user:str, model: str = DEAFAULT_MODEL, temperature=None, max_tokens=None, logit_bias=None, presence_penalty=None):
        """
        Initialize an instance of AI with given settings.

        :param model: N√°zev modelu, kter√Ω bude pou≈æit pro generov√°n√≠ textu. Defaultn√≠ je "gpt-3.5-turbo".
        :param temperature: Hodnota (0-1) ≈ô√≠d√≠c√≠ n√°hodnost generovan√©ho textu. Ni≈æ≈°√≠ hodnoty vedou k v√≠ce deterministick√©mu textu. Defaultn√≠ je 1.
        :param max_tokens: Maxim√°ln√≠ poƒçet token≈Ø, kter√© m≈Ø≈æe kontextov√© okno obsahovat. Defaultnƒõ je maxim√°ln√≠ poƒçet token≈Ø, kter√© m≈Ø≈æe model vygenerovat.
        :param logit_bias: Slovn√≠k pro √∫pravu logit≈Ø konkr√©tn√≠ch token≈Ø, kde kl√≠ƒçe jsou stringy token IDs a hodnoty jsou float bias hodnoty. 
                           Nap≈ô {"50256": -20}. Tokeny lze vyp√°trat pomoc√≠ https://platform.openai.com/tokenizer
        :param presence_penalty: Hodnota (0-2) ud√°vaj√≠c√≠ trest za nep≈ô√≠tomnost token≈Ø ve v√Ωstupu. Defaultn√≠ je 0.
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.logit_bias = logit_bias
        self.presence_penalty = presence_penalty
        self.user = user

    def get_response(self, messages: list[dict[str, str]]):
        """
        Fetch a response from the OpenAI API.
        """
        logging.info(f'Sending: {messages}')

        try:
            # Vytvo≈ôen√≠ slovn√≠ku z atribut≈Ø objektu a filtrov√°n√≠ None hodnot
            params = {key: value for key, value in vars(self).items() if value is not None}

            # P≈ôidej specifick√© parametry, kter√© jsou v≈ædy pot≈ôeba
            params.update({
                "model": self.model,
                "messages": messages
            })

            # Zavolej OpenAI API s dynamicky sestaven√Ωmi parametry
            response = openai.ChatCompletion.create(**params)
        except Exception as e:
            logging.error(f'OpenAI API Error: {e}')
            return "ü§ï An error occurred when calling my brain."

        logging.info(f'Received: {response}')
        return response['choices'][0]['message']['content'].strip()


    # def get_response(self, messages: list[dict[str, str]]):
    #     """
    #     Fetch a response from the OpenAI API.
    #     """
    #     logging.info(f'Sending: {messages}')

    #     try:
    #         response = openai.ChatCompletion.create(
    #             model=self.model,
    #             messages=messages,
    #             temperature=self.temperature,
    #             max_tokens=self.max_tokens,
    #             logit_bias=self.logit_bias,
    #             presence_penalty=self.presence_penalty
    #         )
    #     except Exception as e:
    #         logging.error(f'OpenAI API Error: {e}')
    #         return "An error occurred."

    #     logging.info(f'Received: {response}')
    #     return response['choices'][0]['message']['content'].strip()
            

    def prune_messages(self, messages: list[dict[str, str]], prune_ratio: float) -> list[dict[str, str]]:
        """
        O≈ôe≈æe seznam zpr√°v na z√°kladƒõ zadan√©ho prune_ratio nebo odstran√≠ zpr√°vy tak, aby celkov√Ω poƒçet token≈Ø nep≈ôekroƒçil max_tokens.

        :param messages: Seznam zpr√°v ke zpracov√°n√≠.
        :param prune_ratio: Pomƒõr zpr√°v ke smaz√°n√≠ (0-1) nebo -1 pro odstranƒõn√≠ jedn√© zpr√°vy. Pokud je hodnota 0, bude seznam o≈ôez√°v√°n tak, aby celkov√Ω poƒçet token≈Ø nep≈ôekroƒçil max_tokens.
        :return: O≈ôezan√Ω seznam zpr√°v.
        """
        if prune_ratio:
            prune_length = int(len(messages) * prune_ratio)
            return messages[prune_length:]
        elif prune_ratio == -1:
            return messages.pop(0)
        else:
            while self.num_tokens_from_messages(messages) > self.max_tokens:
                messages.pop(0)
            return messages

    def num_tokens_from_messages(self, messages: list[dict[str, str]], model: str = DEAFAULT_MODEL) -> int:
        """
        Calculate the total number of tokens in the messages based on the model specification.

        :param messages: Seznam zpr√°v ke zpracov√°n√≠.
        :param model: N√°zev modelu pou≈æit√©ho pro v√Ωpoƒçet poƒçtu token≈Ø. Defaultn√≠ je "gpt-3.5-turbo".
        :return: Celkov√Ω poƒçet token≈Ø ve zpr√°v√°ch.
        """
        encoding = tiktoken.encoding_for_model(model)
        tokens_count = 0
        for message in messages:
            content = message['content']
            tokens_count += len(encoding.encode(content))
        return tokens_count